{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc440925-8a1e-4171-b100-3eacb851f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e48b4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/llama_finetune/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data\n"
     ]
    }
   ],
   "source": [
    "# Save df if does not exist\n",
    "csv_file_path = '/Users/ajaykallepalli/data/SQL_df.csv'\n",
    "if not os.path.exists(csv_file_path):\n",
    "    splits = {'train': 'spider/train-00000-of-00001.parquet', 'validation': 'spider/validation-00000-of-00001.parquet'}\n",
    "    df_train = pd.read_parquet(\"hf://datasets/xlangai/spider/\" + splits[\"train\"])\n",
    "    df_train.to_csv(csv_file_path, index=False)\n",
    "    print(\"Downloaded data\")\n",
    "else:\n",
    "    print(\"Already downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84704338",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "hf://datasets/xlangai/spider/spider/test-00000-of-00001.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(csv_file_path):\n\u001b[1;32m      4\u001b[0m     splits \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspider/train-00000-of-00001.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspider/validation-00000-of-00001.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspider/test-00000-of-00001.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m----> 5\u001b[0m     df_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf://datasets/xlangai/spider/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     df_train\u001b[38;5;241m.\u001b[39mto_csv(csv_file_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/llama_finetune/lib/python3.10/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/llama_finetune/lib/python3.10/site-packages/pandas/io/parquet.py:402\u001b[0m, in \u001b[0;36mFastParquetImpl.read\u001b[0;34m(self, path, columns, filters, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m     path \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 402\u001b[0m     parquet_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParquetFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparquet_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parquet_file\u001b[38;5;241m.\u001b[39mto_pandas(columns\u001b[38;5;241m=\u001b[39mcolumns, filters\u001b[38;5;241m=\u001b[39mfilters, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/llama_finetune/lib/python3.10/site-packages/fastparquet/api.py:178\u001b[0m, in \u001b[0;36mParquetFile.__init__\u001b[0;34m(self, fn, verify, open_with, root, sep, fs, pandas_nulls, dtypes)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs \u001b[38;5;241m=\u001b[39m fs\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(fn)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: hf://datasets/xlangai/spider/spider/test-00000-of-00001.parquet"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5239c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load csv if not loaded as a local variable\n",
    "\n",
    "if 'df_train' not in locals():\n",
    "    df_train = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f19a7908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db_id</th>\n",
       "      <th>query</th>\n",
       "      <th>question</th>\n",
       "      <th>query_toks</th>\n",
       "      <th>query_toks_no_value</th>\n",
       "      <th>question_toks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>department_management</td>\n",
       "      <td>SELECT count(*) FROM head WHERE age  &gt;  56</td>\n",
       "      <td>How many heads of the departments are older th...</td>\n",
       "      <td>['SELECT', 'count', '(', '*', ')', 'FROM', 'he...</td>\n",
       "      <td>['select', 'count', '(', '*', ')', 'from', 'he...</td>\n",
       "      <td>['How', 'many', 'heads', 'of', 'the', 'departm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>department_management</td>\n",
       "      <td>SELECT name ,  born_state ,  age FROM head ORD...</td>\n",
       "      <td>List the name, born state and age of the heads...</td>\n",
       "      <td>['SELECT', 'name', ',', 'born_state', ',', 'ag...</td>\n",
       "      <td>['select', 'name', ',', 'born_state', ',', 'ag...</td>\n",
       "      <td>['List', 'the', 'name', ',', 'born', 'state', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>department_management</td>\n",
       "      <td>SELECT creation ,  name ,  budget_in_billions ...</td>\n",
       "      <td>List the creation year, name and budget of eac...</td>\n",
       "      <td>['SELECT', 'creation', ',', 'name', ',', 'budg...</td>\n",
       "      <td>['select', 'creation', ',', 'name', ',', 'budg...</td>\n",
       "      <td>['List', 'the', 'creation', 'year', ',', 'name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>department_management</td>\n",
       "      <td>SELECT max(budget_in_billions) ,  min(budget_i...</td>\n",
       "      <td>What are the maximum and minimum budget of the...</td>\n",
       "      <td>['SELECT', 'max', '(', 'budget_in_billions', '...</td>\n",
       "      <td>['select', 'max', '(', 'budget_in_billions', '...</td>\n",
       "      <td>['What', 'are', 'the', 'maximum', 'and', 'mini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>department_management</td>\n",
       "      <td>SELECT avg(num_employees) FROM department WHER...</td>\n",
       "      <td>What is the average number of employees of the...</td>\n",
       "      <td>['SELECT', 'avg', '(', 'num_employees', ')', '...</td>\n",
       "      <td>['select', 'avg', '(', 'num_employees', ')', '...</td>\n",
       "      <td>['What', 'is', 'the', 'average', 'number', 'of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   db_id                                              query  \\\n",
       "0  department_management         SELECT count(*) FROM head WHERE age  >  56   \n",
       "1  department_management  SELECT name ,  born_state ,  age FROM head ORD...   \n",
       "2  department_management  SELECT creation ,  name ,  budget_in_billions ...   \n",
       "3  department_management  SELECT max(budget_in_billions) ,  min(budget_i...   \n",
       "4  department_management  SELECT avg(num_employees) FROM department WHER...   \n",
       "\n",
       "                                            question  \\\n",
       "0  How many heads of the departments are older th...   \n",
       "1  List the name, born state and age of the heads...   \n",
       "2  List the creation year, name and budget of eac...   \n",
       "3  What are the maximum and minimum budget of the...   \n",
       "4  What is the average number of employees of the...   \n",
       "\n",
       "                                          query_toks  \\\n",
       "0  ['SELECT', 'count', '(', '*', ')', 'FROM', 'he...   \n",
       "1  ['SELECT', 'name', ',', 'born_state', ',', 'ag...   \n",
       "2  ['SELECT', 'creation', ',', 'name', ',', 'budg...   \n",
       "3  ['SELECT', 'max', '(', 'budget_in_billions', '...   \n",
       "4  ['SELECT', 'avg', '(', 'num_employees', ')', '...   \n",
       "\n",
       "                                 query_toks_no_value  \\\n",
       "0  ['select', 'count', '(', '*', ')', 'from', 'he...   \n",
       "1  ['select', 'name', ',', 'born_state', ',', 'ag...   \n",
       "2  ['select', 'creation', ',', 'name', ',', 'budg...   \n",
       "3  ['select', 'max', '(', 'budget_in_billions', '...   \n",
       "4  ['select', 'avg', '(', 'num_employees', ')', '...   \n",
       "\n",
       "                                       question_toks  \n",
       "0  ['How', 'many', 'heads', 'of', 'the', 'departm...  \n",
       "1  ['List', 'the', 'name', ',', 'born', 'state', ...  \n",
       "2  ['List', 'the', 'creation', 'year', ',', 'name...  \n",
       "3  ['What', 'are', 'the', 'maximum', 'and', 'mini...  \n",
       "4  ['What', 'is', 'the', 'average', 'number', 'of...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "550d01c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['department_management', 'farm', 'student_assessment', 'bike_1',\n",
       "       'book_2', 'musical', 'twitter_1', 'product_catalog', 'flight_1',\n",
       "       'allergy_1', 'store_1', 'journal_committee',\n",
       "       'customers_card_transactions', 'race_track', 'coffee_shop',\n",
       "       'chinook_1', 'insurance_fnol', 'medicine_enzyme_interaction',\n",
       "       'university_basketball', 'phone_1', 'match_season', 'climbing',\n",
       "       'body_builder', 'election_representative', 'apartment_rentals',\n",
       "       'game_injury', 'soccer_1', 'performance_attendance', 'college_2',\n",
       "       'debate', 'insurance_and_eClaims', 'customers_and_invoices',\n",
       "       'wedding', 'theme_gallery', 'epinions_1', 'riding_club', 'gymnast',\n",
       "       'small_bank_1', 'browser_web', 'wrestler', 'school_finance',\n",
       "       'protein_institute', 'cinema', 'products_for_hire', 'phone_market',\n",
       "       'gas_company', 'party_people', 'pilot_record',\n",
       "       'cre_Doc_Control_Systems', 'company_1', 'local_govt_in_alabama',\n",
       "       'formula_1', 'machine_repair', 'entrepreneur', 'perpetrator',\n",
       "       'csu_1', 'candidate_poll', 'movie_1', 'county_public_safety',\n",
       "       'inn_1', 'local_govt_mdm', 'party_host', 'storm_record',\n",
       "       'election', 'news_report', 'restaurant_1', 'customer_deliveries',\n",
       "       'icfp_1', 'sakila_1', 'loan_1', 'behavior_monitoring',\n",
       "       'assets_maintenance', 'station_weather', 'college_1',\n",
       "       'sports_competition', 'manufacturer', 'hr_1', 'music_1',\n",
       "       'baseball_1', 'mountain_photos', 'program_share', 'e_learning',\n",
       "       'insurance_policies', 'hospital_1', 'ship_mission', 'student_1',\n",
       "       'company_employee', 'film_rank', 'cre_Doc_Tracking_DB', 'club_1',\n",
       "       'tracking_grants_for_research', 'network_2',\n",
       "       'decoration_competition', 'document_management', 'company_office',\n",
       "       'solvency_ii', 'entertainment_awards',\n",
       "       'customers_campaigns_ecommerce', 'college_3', 'department_store',\n",
       "       'aircraft', 'local_govt_and_lot', 'school_player', 'store_product',\n",
       "       'soccer_2', 'device', 'cre_Drama_Workshop_Groups', 'music_2',\n",
       "       'manufactory_1', 'tracking_software_problems', 'shop_membership',\n",
       "       'voter_2', 'products_gen_characteristics', 'swimming', 'railway',\n",
       "       'customers_and_products_contacts', 'dorm_1', 'customer_complaints',\n",
       "       'workshop_paper', 'tracking_share_transactions', 'cre_Theme_park',\n",
       "       'game_1', 'customers_and_addresses', 'music_4', 'roller_coaster',\n",
       "       'ship_1', 'city_record', 'e_government', 'school_bus',\n",
       "       'flight_company', 'cre_Docs_and_Epenses', 'scientist_1', 'wine_1',\n",
       "       'train_station', 'driving_school', 'activity_1', 'flight_4',\n",
       "       'tracking_orders', 'architecture', 'culture_company'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding db_schema to the training data\n",
    "# Getting unique db id list\n",
    "unique_dbs = df_train['db_id'].unique()\n",
    "unique_dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a965ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a db with db ID and db schema as columns\n",
    "df_schemas = pd.DataFrame(columns=['db_id', 'db_schema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a64f440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ajaykallepalli/data/spider/database/department_management/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/farm/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/student_assessment/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/bike_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/book_2/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/musical/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/twitter_1/schema.sql\n",
      "Schema file not found for twitter_1\n",
      "/Users/ajaykallepalli/data/spider/database/product_catalog/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/flight_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/allergy_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/store_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/journal_committee/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/customers_card_transactions/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/race_track/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/coffee_shop/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/chinook_1/schema.sql\n",
      "Schema file not found for chinook_1\n",
      "/Users/ajaykallepalli/data/spider/database/insurance_fnol/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/medicine_enzyme_interaction/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/university_basketball/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/phone_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/match_season/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/climbing/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/body_builder/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/election_representative/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/apartment_rentals/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/game_injury/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/soccer_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/performance_attendance/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/college_2/schema.sql\n",
      "Schema file not found for college_2\n",
      "/Users/ajaykallepalli/data/spider/database/debate/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/insurance_and_eClaims/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/customers_and_invoices/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/wedding/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/theme_gallery/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/epinions_1/schema.sql\n",
      "Schema file not found for epinions_1\n",
      "/Users/ajaykallepalli/data/spider/database/riding_club/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/gymnast/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/small_bank_1/schema.sql\n",
      "Schema file not found for small_bank_1\n",
      "/Users/ajaykallepalli/data/spider/database/browser_web/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/wrestler/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/school_finance/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/protein_institute/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/cinema/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/products_for_hire/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/phone_market/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/gas_company/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/party_people/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/pilot_record/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/cre_Doc_Control_Systems/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/company_1/schema.sql\n",
      "Schema file not found for company_1\n",
      "/Users/ajaykallepalli/data/spider/database/local_govt_in_alabama/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/formula_1/schema.sql\n",
      "Schema file not found for formula_1\n",
      "/Users/ajaykallepalli/data/spider/database/machine_repair/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/entrepreneur/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/perpetrator/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/csu_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/candidate_poll/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/movie_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/county_public_safety/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/inn_1/schema.sql\n",
      "Schema file not found for inn_1\n",
      "/Users/ajaykallepalli/data/spider/database/local_govt_mdm/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/party_host/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/storm_record/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/election/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/news_report/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/restaurant_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/customer_deliveries/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/icfp_1/schema.sql\n",
      "Schema file not found for icfp_1\n",
      "/Users/ajaykallepalli/data/spider/database/sakila_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/loan_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/behavior_monitoring/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/assets_maintenance/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/station_weather/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/college_1/schema.sql\n",
      "Schema file not found for college_1\n",
      "/Users/ajaykallepalli/data/spider/database/sports_competition/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/manufacturer/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/hr_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/music_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/baseball_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/mountain_photos/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/program_share/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/e_learning/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/insurance_policies/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/hospital_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/ship_mission/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/student_1/schema.sql\n",
      "Schema file not found for student_1\n",
      "/Users/ajaykallepalli/data/spider/database/company_employee/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/film_rank/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/cre_Doc_Tracking_DB/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/club_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/tracking_grants_for_research/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/network_2/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/decoration_competition/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/document_management/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/company_office/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/solvency_ii/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/entertainment_awards/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/customers_campaigns_ecommerce/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/college_3/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/department_store/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/aircraft/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/local_govt_and_lot/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/school_player/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/store_product/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/soccer_2/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/device/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/cre_Drama_Workshop_Groups/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/music_2/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/manufactory_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/tracking_software_problems/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/shop_membership/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/voter_2/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/products_gen_characteristics/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/swimming/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/railway/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/customers_and_products_contacts/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/dorm_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/customer_complaints/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/workshop_paper/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/tracking_share_transactions/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/cre_Theme_park/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/game_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/customers_and_addresses/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/music_4/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/roller_coaster/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/ship_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/city_record/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/e_government/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/school_bus/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/flight_company/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/cre_Docs_and_Epenses/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/scientist_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/wine_1/schema.sql\n",
      "Schema file not found for wine_1\n",
      "/Users/ajaykallepalli/data/spider/database/train_station/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/driving_school/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/activity_1/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/flight_4/schema.sql\n",
      "Schema file not found for flight_4\n",
      "/Users/ajaykallepalli/data/spider/database/tracking_orders/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/architecture/schema.sql\n",
      "/Users/ajaykallepalli/data/spider/database/culture_company/schema.sql\n"
     ]
    }
   ],
   "source": [
    "schema_location = '/Users/ajaykallepalli/data/spider/database/'\n",
    "schema_name = 'schema.sql'\n",
    "for uniq_db in unique_dbs:\n",
    "    schema_file = os.path.join(schema_location, uniq_db, schema_name)\n",
    "    print(schema_file)\n",
    "    if os.path.exists(schema_file):\n",
    "        with open(schema_file, 'r') as f:\n",
    "            schema_content = f.read()\n",
    "        df_schemas = pd.concat([df_schemas, pd.DataFrame({'db_id': [uniq_db], 'db_schema': [schema_content]})], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"Schema file not found for {uniq_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eace2286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schemas CSV file saved to /Users/ajaykallepalli/data/df_schemas.csv\n",
      "Shape of df_schemas: (127, 2)\n"
     ]
    }
   ],
   "source": [
    "# Save the df_schemas DataFrame to a CSV file\n",
    "csv_schemas_file_path = '/Users/ajaykallepalli/data/df_schemas.csv'\n",
    "\n",
    "if not os.path.exists(csv_schemas_file_path):\n",
    "    df_schemas.to_csv(csv_schemas_file_path, index=False)\n",
    "    print(f\"Schemas CSV file saved to {csv_schemas_file_path}\")\n",
    "else:\n",
    "    print(f\"Schemas CSV file already exists at {csv_schemas_file_path}\")\n",
    "\n",
    "# Print the shape of df_schemas\n",
    "print(\"Shape of df_schemas:\", df_schemas.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd92587d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db_id</th>\n",
       "      <th>db_schema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>department_management</td>\n",
       "      <td>PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>farm</td>\n",
       "      <td>\\nPRAGMA foreign_keys = ON;\\n\\nCREATE TABLE \"c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>student_assessment</td>\n",
       "      <td>PRAGMA foreign_keys = ON;\\n\\nCREATE TABLE Addr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bike_1</td>\n",
       "      <td>PRAGMA foreign_keys=OFF;\\nBEGIN TRANSACTION;\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>book_2</td>\n",
       "      <td>PRAGMA foreign_keys = ON;\\n\\nCREATE TABLE \"pub...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   db_id                                          db_schema\n",
       "0  department_management  PRAGMA foreign_keys=ON;\\nBEGIN TRANSACTION;\\nC...\n",
       "1                   farm  \\nPRAGMA foreign_keys = ON;\\n\\nCREATE TABLE \"c...\n",
       "2     student_assessment  PRAGMA foreign_keys = ON;\\n\\nCREATE TABLE Addr...\n",
       "3                 bike_1  PRAGMA foreign_keys=OFF;\\nBEGIN TRANSACTION;\\n...\n",
       "4                 book_2  PRAGMA foreign_keys = ON;\\n\\nCREATE TABLE \"pub..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_schemas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f62fa1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 984 rows have missing schemas.\n",
      "Unique db_ids with missing schemas: ['twitter_1' 'chinook_1' 'college_2' 'epinions_1' 'small_bank_1'\n",
      " 'company_1' 'formula_1' 'inn_1' 'icfp_1' 'college_1' 'student_1' 'wine_1'\n",
      " 'flight_4']\n",
      "Shape of df_train after merge: (7000, 7)\n",
      "Columns in df_train: Index(['db_id', 'query', 'question', 'query_toks', 'query_toks_no_value',\n",
      "       'question_toks', 'db_schema'],\n",
      "      dtype='object')\n",
      "Null value counts:\n",
      " db_id                    0\n",
      "query                    0\n",
      "question                 0\n",
      "query_toks               0\n",
      "query_toks_no_value      0\n",
      "question_toks            0\n",
      "db_schema              984\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Joining the df_schema to the train data.\n",
    "df_train = df_train.merge(df_schemas, on='db_id', how='left')\n",
    "\n",
    "# Check for any missing schemas\n",
    "missing_schemas = df_train[df_train['db_schema'].isnull()]\n",
    "if not missing_schemas.empty:\n",
    "    print(f\"Warning: {len(missing_schemas)} rows have missing schemas.\")\n",
    "    print(\"Unique db_ids with missing schemas:\", missing_schemas['db_id'].unique())\n",
    "\n",
    "# Verify the merge\n",
    "print(\"Shape of df_train after merge:\", df_train.shape)\n",
    "print(\"Columns in df_train:\", df_train.columns)\n",
    "\n",
    "# Check for null values in the merged dataframe\n",
    "null_counts = df_train.isnull().sum()\n",
    "print(\"Null value counts:\\n\", null_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5828703c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to /Users/ajaykallepalli/data/df_schema_train.csv\n",
      "Shape of df_schema_train after removing NA values: (6016, 7)\n"
     ]
    }
   ],
   "source": [
    "# Remove all NA values from df_train\n",
    "df_schema_train = df_train.dropna()\n",
    "# Group by 'db_id' and keep only the first 20 rows for each schema\n",
    "df_schema_train = df_schema_train.groupby('db_id').apply(lambda x: x.head(20)).reset_index(drop=True)\n",
    "\n",
    "# Print the shape of the new dataframe after keeping only 20 of each schema\n",
    "print(\"Shape of df_schema_train after keeping 20 of each schema:\", df_schema_train.shape)\n",
    "\n",
    "# Print the number of unique schemas\n",
    "print(\"Number of unique schemas:\", df_schema_train['db_id'].nunique())\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_schema_file_path = '/Users/ajaykallepalli/data/df_schema_train.csv'\n",
    "\n",
    "# Save the new dataframe to the same folder\n",
    "if not os.path.exists(csv_schema_file_path):\n",
    "    df_schema_train.to_csv(csv_schema_file_path, index=False)\n",
    "    print(f\"File saved to {csv_schema_file_path}\")\n",
    "else:\n",
    "    print(f\"File already exists at {csv_schema_file_path}\")\n",
    "\n",
    "# Print the shape of the new dataframe\n",
    "print(\"Shape of df_schema_train after removing NA values:\", df_schema_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da6908c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1 saved to /Users/ajaykallepalli/data/df_schema_train_1.csv\n",
      "Chunk 2 saved to /Users/ajaykallepalli/data/df_schema_train_2.csv\n",
      "Chunk 3 saved to /Users/ajaykallepalli/data/df_schema_train_3.csv\n",
      "Chunk 4 saved to /Users/ajaykallepalli/data/df_schema_train_4.csv\n",
      "Chunk 5 saved to /Users/ajaykallepalli/data/df_schema_train_5.csv\n",
      "Chunk 6 saved to /Users/ajaykallepalli/data/df_schema_train_6.csv\n",
      "Chunk 7 saved to /Users/ajaykallepalli/data/df_schema_train_7.csv\n",
      "Chunk 8 saved to /Users/ajaykallepalli/data/df_schema_train_8.csv\n",
      "Chunk 9 saved to /Users/ajaykallepalli/data/df_schema_train_9.csv\n",
      "Chunk 10 saved to /Users/ajaykallepalli/data/df_schema_train_10.csv\n",
      "All chunks have been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of rows per chunk\n",
    "chunk_size = len(df_schema_train) // 10\n",
    "\n",
    "# Base path for the CSV files\n",
    "base_path = '/Users/ajaykallepalli/data/df_schema_train'\n",
    "\n",
    "# Chunk the dataframe and save to CSV files\n",
    "for i in range(10):\n",
    "    start = i * chunk_size\n",
    "    end = (i + 1) * chunk_size if i < 9 else len(df_schema_train)\n",
    "    \n",
    "    chunk = df_schema_train.iloc[start:end]\n",
    "    \n",
    "    # Create the file name with the suffix _1\n",
    "    file_name = f\"{base_path}_{i+1}.csv\"\n",
    "    \n",
    "    # Save the chunk to CSV\n",
    "    chunk.to_csv(file_name, index=False)\n",
    "    print(f\"Chunk {i+1} saved to {file_name}\")\n",
    "\n",
    "print(\"All chunks have been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9353eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "print(\"All chunks have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8729442",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_instruction = 0\n",
    "max_schema = 0\n",
    "max_input = 0\n",
    "max_output = 0\n",
    "max_total = 0\n",
    "\n",
    "instructions = df_schema_train[\"instruction\"]\n",
    "schemas = df_schema_train[\"db_schema\"]\n",
    "inputs = df_schema_train[\"question\"]\n",
    "outputs = df_schema_train[\"query\"]\n",
    "\n",
    "for instruction, schema, input, output in zip(instructions, schemas, inputs, outputs):\n",
    "    # Count the number of tokens in the input data fields\n",
    "    instruction_tokens = tokenizer.encode_plus(instruction, add_special_tokens=False, max_length=None)[\"input_ids\"]\n",
    "    schema_tokens = tokenizer.encode_plus(schema, add_special_tokens=False, max_length=None)[\"input_ids\"]\n",
    "    input_tokens = tokenizer.encode_plus(input, add_special_tokens=False, max_length=None)[\"input_ids\"]\n",
    "    output_tokens = tokenizer.encode_plus(output, add_special_tokens=False, max_length=None)[\"input_ids\"]\n",
    "    total_tokens = tokenizer.encode_plus(f\"{instruction} {schema} {input} {output}\", add_special_tokens=False, max_length=None)[\"input_ids\"]\n",
    "\n",
    "    max_instruction = max(max_instruction, len(instruction_tokens))\n",
    "    max_schema = max(max_schema, len(schema_tokens))\n",
    "    max_input = max(max_input, len(input_tokens))\n",
    "    max_output = max(max_output, len(output_tokens))\n",
    "    max_total = max(max_total, len(total_tokens))\n",
    "\n",
    "# Display the results\n",
    "print(f\"\\nMaximum token counts:\")\n",
    "print(f\"Instruction: {max_instruction}\")\n",
    "print(f\"Schema: {max_schema}\")\n",
    "print(f\"Input: {max_input}\")\n",
    "print(f\"Output: {max_output}\")\n",
    "print(f\"Total: {max_total}\")\n",
    "\n",
    "# Add a small buffer to the maximum token count\n",
    "buffer = 10\n",
    "# max_seq_length can be set up to 2x the default context length\n",
    "# of the base model because Unsloth supports RoPE Scaling internally.\n",
    "# Here, we auto-configure this length based on input data analysis.\n",
    "max_seq_length = max_total + buffer\n",
    "\n",
    "# Display the table header\n",
    "table_title = \"Training Data Token Counts\"\n",
    "print(f\"\\n{table_title:-^70}\")\n",
    "print(f\"{'Measure':<14}{'Instruction':<14}{'Schema':<14}{'Input':<14}{'Output':<14}{'Total':<14}\")\n",
    "\n",
    "# Display token counts in tabular form\n",
    "print(f\"{'Maximums':<14}{max_instruction:<14}{max_schema:<14}{max_input:<14}{max_output:<14}{max_total:<14}\")\n",
    "print(f\"{'Max Seq Len':<14}{'':<14}{'':<14}{'':<14}{'':<14}{max_seq_length:<14}\\n\")\n",
    "\n",
    "print(f\"Set max_seq_length in FastLanguageModel to {max_seq_length} to handle the maximum number of tokens required by the input training data (Total Maximum + Buffer).\")\n",
    "\n",
    "# Add a small buffer to the maximum token count\n",
    "buffer = 10\n",
    "# max_seq_length can be set up to 2x the default context length\n",
    "# of the base model because Unsloth supports RoPE Scaling internally.\n",
    "# Here, we auto-configure this length based on input data analysis.\n",
    "max_seq_length = max_qna + buffer\n",
    "\n",
    "# Display the table header\n",
    "table_title = \"Training Data Token Counts\"\n",
    "print(f\"\\n{table_title:-^70}\")\n",
    "print(f\"{'Measure':<14}{'Question':<14}{'Answer':<14}{'Combined':<14}\")\n",
    "\n",
    "# Display token counts in tabular form\n",
    "print(f\"{'Maximums':<14}{max_q:<14}{max_a:<14}{max_qna:<14}\")\n",
    "print(f\"{'Max Seq Len':<14}{'':<14}{'':<14}{max_seq_length:<14}\\n\")\n",
    "\n",
    "print(f\"Set max_seq_length in FastLanguageModel to {max_seq_length} to handle the maximum number of tokens required by the input training data (Combined Maximum + Buffer).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c2860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
